{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11e42824-e60d-42b0-bdb8-ca2f69ca219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c26855f3-1ea4-4f5d-903e-d91afbb47e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore, Entity, FeatureView, Feature, ValueType, FileSource, RepoConfig\n",
    "from feast.infra.offline_stores.file_source import SavedDatasetFileStorage\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bbd5f51-f996-47f7-9f58-5cdd293c359c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"cd ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3228acc-662d-4330-b642-230400076b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"cd /usr/src/datanoob/repository && feast apply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a35e5d3-0e0f-4c33-a7ab-33df66756221",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"../repository/feature_store\"\n",
    "\n",
    "# fs = FeatureStore(config=RepoConfig(registry=f\"{repo_path}/registry.db\", \n",
    "#                                     project=\"mlops_regression\", \n",
    "#                                     provider=\"local\",\n",
    "#                                     # online_store={\"path\":\"../repository/feature_store/online_store.db\"},\n",
    "#                                     # online_store=dict(path=\"../repository/feature_store/online_store.db\"),\n",
    "#                                     # online_store=\"../repository/feature_store/online_store.db\"\n",
    "#                                    )\n",
    "#                  )\n",
    "\n",
    "repo_path = \"../repository\"\n",
    "fs = FeatureStore(repo_path=f\"{repo_path}\")\n",
    "\n",
    "house = Entity(name=\"house_id\", value_type=ValueType.INT64, description=\"house_id\",)\n",
    "\n",
    "toxic_monthly_stats = FileSource(\n",
    "    # path=f\"{repo_path}/toxic.parquet\",\n",
    "    path=f\"usr/src/datanoob/repository/feature_store/toxic.parquet\",\n",
    "    event_timestamp_column=\"event_timestamp\",\n",
    "    # created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "toxic_monthly_stats_view = FeatureView(\n",
    "    name=\"toxic_monthly_stats\",\n",
    "    entities=[\"house_id\"],\n",
    "    ttl=timedelta(seconds=86400 * 1),\n",
    "    features=[\n",
    "        Feature(name=\"NOX\", dtype=ValueType.FLOAT),\n",
    "        # Feature(name=\"acc_rate\", dtype=ValueType.FLOAT),\n",
    "        # Feature(name=\"avg_daily_trips\", dtype=ValueType.INT64),\n",
    "    ],\n",
    "    batch_source=toxic_monthly_stats,\n",
    ")\n",
    "fs.apply([toxic_monthly_stats_view, house]) # register entity and feature view\n",
    "# fs.materialize(\n",
    "#     start_date=datetime.utcnow() - timedelta(hours=3), end_date=datetime.utcnow() - timedelta(minutes=10)\n",
    "# )\n",
    "# fs.materialize_incremental(end_date=datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "881389cf-ba7d-4451-a2a1-520980ad2440",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views to \u001b[1m\u001b[32m2022-07-16 20:01:45+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mtoxic_monthly_stats\u001b[0m from \u001b[1m\u001b[32m2022-07-15 20:01:45+00:00\u001b[0m to \u001b[1m\u001b[32m2022-07-16 20:01:45+00:00\u001b[0m:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "/usr/src/datanoob/notebook/usr/src/datanoob/repository/feature_store/toxic.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterialize_incremental\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/feast/usage.py:269\u001b[0m, in \u001b[0;36mlog_exceptions_and_usage.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m ctx\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mupdate(attrs)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mexception:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;66;03m# exception was already recorded\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/feast/feature_store.py:1256\u001b[0m, in \u001b[0;36mFeatureStore.materialize_incremental\u001b[0;34m(self, end_date, feature_views)\u001b[0m\n\u001b[1;32m   1253\u001b[0m start_date \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mmake_tzaware(start_date)\n\u001b[1;32m   1254\u001b[0m end_date \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mmake_tzaware(end_date)\n\u001b[0;32m-> 1256\u001b[0m \u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterialize_single_feature_view\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_view\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_view\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_registry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtqdm_builder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_builder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry\u001b[38;5;241m.\u001b[39mapply_materialization(\n\u001b[1;32m   1267\u001b[0m     feature_view, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject, start_date, end_date,\n\u001b[1;32m   1268\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/feast/infra/passthrough_provider.py:191\u001b[0m, in \u001b[0;36mPassthroughProvider.materialize_single_feature_view\u001b[0;34m(self, config, feature_view, start_date, end_date, registry, project, tqdm_builder)\u001b[0m\n\u001b[1;32m    173\u001b[0m (\n\u001b[1;32m    174\u001b[0m     join_key_columns,\n\u001b[1;32m    175\u001b[0m     feature_name_columns,\n\u001b[1;32m    176\u001b[0m     timestamp_field,\n\u001b[1;32m    177\u001b[0m     created_timestamp_column,\n\u001b[1;32m    178\u001b[0m ) \u001b[38;5;241m=\u001b[39m _get_column_names(feature_view, entities)\n\u001b[1;32m    180\u001b[0m offline_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffline_store\u001b[38;5;241m.\u001b[39mpull_latest_from_table_or_query(\n\u001b[1;32m    181\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    182\u001b[0m     data_source\u001b[38;5;241m=\u001b[39mfeature_view\u001b[38;5;241m.\u001b[39mbatch_source,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m     end_date\u001b[38;5;241m=\u001b[39mend_date,\n\u001b[1;32m    189\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43moffline_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_view\u001b[38;5;241m.\u001b[39mbatch_source\u001b[38;5;241m.\u001b[39mfield_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     table \u001b[38;5;241m=\u001b[39m _run_field_mapping(table, feature_view\u001b[38;5;241m.\u001b[39mbatch_source\u001b[38;5;241m.\u001b[39mfield_mapping)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/feast/infra/offline_stores/offline_store.py:125\u001b[0m, in \u001b[0;36mRetrievalJob.to_arrow\u001b[0;34m(self, validation_reference)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03mReturn dataset as pyarrow Table synchronously\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    validation_reference: If provided resulting dataset will be validated against this reference profile.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_demand_feature_views \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validation_reference:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_arrow_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m features_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_df_internal()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_demand_feature_views:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/feast/usage.py:269\u001b[0m, in \u001b[0;36mlog_exceptions_and_usage.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m ctx\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mupdate(attrs)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mexception:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;66;03m# exception was already recorded\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/feast/infra/offline_stores/file.py:85\u001b[0m, in \u001b[0;36mFileRetrievalJob._to_arrow_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@log_exceptions_and_usage\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_arrow_internal\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Only execute the evaluation function to build the final historical retrieval dataframe at the last moment.\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pyarrow\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(df)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/feast/infra/offline_stores/file.py:293\u001b[0m, in \u001b[0;36mFileOfflineStore.pull_latest_from_table_or_query.<locals>.evaluate_offline_job\u001b[0;34m()\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_offline_job\u001b[39m():\n\u001b[0;32m--> 293\u001b[0m     source_df \u001b[38;5;241m=\u001b[39m \u001b[43m_read_datasource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m     source_df \u001b[38;5;241m=\u001b[39m _normalize_timestamp(\n\u001b[1;32m    296\u001b[0m         source_df, timestamp_field, created_timestamp_column\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    299\u001b[0m     source_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(source_df\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/feast/infra/offline_stores/file.py:483\u001b[0m, in \u001b[0;36m_read_datasource\u001b[0;34m(data_source)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_datasource\u001b[39m(data_source) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m dd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m    473\u001b[0m     storage_options \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    474\u001b[0m         {\n\u001b[1;32m    475\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    481\u001b[0m     )\n\u001b[0;32m--> 483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py:330\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, gather_statistics, ignore_metadata_file, metadata_task_size, split_row_groups, chunksize, aggregate_files, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_parquet options require gather_statistics=True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    328\u001b[0m     gather_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m read_metadata_result \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgather_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# compatibility with a user-defined engine.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m meta, statistics, parts, index \u001b[38;5;241m=\u001b[39m read_metadata_result[:\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py:371\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_metadata\u001b[39m(\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \n\u001b[1;32m    370\u001b[0m     \u001b[38;5;66;03m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     dataset_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_dataset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Stage 2: Generate output `meta`\u001b[39;00m\n\u001b[1;32m    387\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dd_meta(dataset_info)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py:909\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# Final \"catch-all\" pyarrow.dataset call\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 909\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mpa_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscover\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_dataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# At this point, we know if `split_row_groups` should be\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# set to `True` by default.  If the user has not specified\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# this option, we will only collect statistics if there is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# interpreted as an indication that metadata overhead should\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;66;03m# be avoided at all costs.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gather_statistics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyarrow/dataset.py:683\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n\u001b[0;32m--> 683\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(elem, Dataset) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _union_dataset(source, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyarrow/dataset.py:423\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    420\u001b[0m partitioning \u001b[38;5;241m=\u001b[39m _ensure_partitioning(partitioning)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 423\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_multiple_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m _ensure_single_source(source, filesystem)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyarrow/dataset.py:342\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[0;34m(paths, filesystem)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mNotFound:\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(info\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mDirectory:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m(\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m points to a directory, but only file paths are \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupported. To construct a nested or union dataset pass \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma list of dataset objects instead.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(info\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m    348\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /usr/src/datanoob/notebook/usr/src/datanoob/repository/feature_store/toxic.parquet"
     ]
    }
   ],
   "source": [
    "fs.materialize_incremental(end_date=datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23ab2335-14ad-4e6c-9ef7-87faabf3dd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'house_id': [2], 'NOX': [None]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_response = fs.get_online_features(\n",
    "    features=[\n",
    "        \"toxic_monthly_stats:NOX\",\n",
    "        # \"driver_hourly_stats:acc_rate\",\n",
    "        # \"driver_hourly_stats:avg_daily_trips\",\n",
    "    ],\n",
    "    entity_rows=[{\"house_id\": 2}],\n",
    ")\n",
    "online_response_dict = online_response.to_dict()\n",
    "online_response_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5df86a9a-617a-427c-b748-4981836e81ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>MEDV</th>\n",
       "      <th>load_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>21.6</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>34.7</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>33.4</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>36.2</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>502</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>22.4</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>503</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>20.6</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>504</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>23.9</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>505</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>506</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     house_id  ...     load_dt\n",
       "0           1  ...  2022-04-01\n",
       "1           2  ...  2022-04-01\n",
       "2           3  ...  2022-04-01\n",
       "3           4  ...  2022-04-01\n",
       "4           5  ...  2022-04-01\n",
       "..        ...  ...         ...\n",
       "501       502  ...  2022-04-01\n",
       "502       503  ...  2022-04-01\n",
       "503       504  ...  2022-04-01\n",
       "504       505  ...  2022-04-01\n",
       "505       506  ...  2022-04-01\n",
       "\n",
       "[506 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting our FeatureStore\n",
    "repo_path = \"../repository\"\n",
    "store = FeatureStore(repo_path=f\"{repo_path}\")\n",
    "\n",
    "# Reading our targets as an entity DataFrame\n",
    "entity_df = pd.read_parquet(path=f\"{repo_path}/feature_store/target.parquet\")\n",
    "entity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09724a86-76d4-4eac-9026-0e28bd4268f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/feast/feature_store.py:1115: RuntimeWarning: Saving dataset is an experimental feature. This API is unstable and it could and most probably will be changed in the future. We do not guarantee that future changes will maintain backward compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_data = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"toxic_monthly_stats:NOX\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Storing the dataset as a local file: your path\n",
    "dataset = store.create_saved_dataset(\n",
    "    from_=training_data,\n",
    "    name=\"mlops_regression_dataset\",\n",
    "    storage=SavedDatasetFileStorage(\"../repository/metadata/training_dataset.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee34e11-3458-40c7-b913-2bc0224ea90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs get repo_pat = ymal\n",
    "# FileSource path = full path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b038081-1528-4bbf-8be3-756060e494d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf.duration_pb2 import Duration\n",
    "from feast import Entity, Feature, FeatureView, FileSource, ValueType\n",
    "\n",
    "repo_path = \"../repository\"\n",
    "fs = FeatureStore(repo_path=f\"{repo_path}\")\n",
    "\n",
    "# Declaring an entity for the dataset\n",
    "house = Entity(name=\"house_id\", value_type=ValueType.INT64, description=\"house_id\",)\n",
    "\n",
    "toxic_monthly_stats = FileSource(\n",
    "    # path=f\"{repo_path}/toxic.parquet\",\n",
    "    path=\"/usr/src/datanoob/repository/feature_store/toxic.parquet\",\n",
    "    event_timestamp_column=\"event_timestamp\",\n",
    ")\n",
    "\n",
    "toxic_monthly_stats_view = FeatureView(\n",
    "    name=\"toxic_monthly_stats\",\n",
    "    entities=[\"house_id\"],\n",
    "    # ttl=timedelta(seconds=86400 * 1),\n",
    "    ttl=Duration(seconds=86400 * 60),\n",
    "    features=[\n",
    "        Feature(name=\"NOX\", dtype=ValueType.FLOAT),\n",
    "    ],\n",
    "    batch_source=toxic_monthly_stats,\n",
    ")\n",
    "\n",
    "# Declaring the source of the targets\n",
    "target_source = FileSource(\n",
    "    path=\"/usr/src/datanoob/repository/feature_store/target.parquet\", \n",
    "    created_timestamp_column=\"event_timestamp\")\n",
    "\n",
    "# Defining the targets\n",
    "target_fv = FeatureView(\n",
    "    name=\"target_feature_view\",\n",
    "    entities=[\"house_id\"],\n",
    "    ttl=Duration(seconds=86400 * 60),\n",
    "    features=[\n",
    "        Feature(name=\"MEDV\", dtype=ValueType.INT32)        \n",
    "        ],    \n",
    "    batch_source=target_source\n",
    ")\n",
    "fs.apply([toxic_monthly_stats_view, house, target_fv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01dfa39e-ee77-4aac-89a1-15d8a71cdb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/feast/feature_store.py:1115: RuntimeWarning: Saving dataset is an experimental feature. This API is unstable and it could and most probably will be changed in the future. We do not guarantee that future changes will maintain backward compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_data = fs.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"toxic_monthly_stats:NOX\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Storing the dataset as a local file: your path\n",
    "dataset = fs.create_saved_dataset(\n",
    "    from_=training_data,\n",
    "    name=\"mlops_regression_dataset\",\n",
    "    storage=SavedDatasetFileStorage(\"../repository/metadata/training_dataset.parquet\"),\n",
    "    # storage=SavedDatasetFileStorage(\"../repository/feature_store/training_dataset.parquet\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "edae0cce-61c2-4275-8c83-b6c52a65783d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>MEDV</th>\n",
       "      <th>load_dt</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>345</td>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>31.2</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>344</td>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>23.9</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>343</td>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>16.5</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>0.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>342</td>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>32.7</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>0.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>163</td>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>0.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>162</td>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>0.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>161</td>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>0.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>174</td>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>23.6</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>506</td>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>0.573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     house_id  ...    NOX\n",
       "0           1  ...  0.538\n",
       "344       345  ...  0.484\n",
       "343       344  ...  0.484\n",
       "342       343  ...  0.518\n",
       "341       342  ...  0.442\n",
       "..        ...  ...    ...\n",
       "162       163  ...  0.605\n",
       "161       162  ...  0.605\n",
       "160       161  ...  0.605\n",
       "173       174  ...  0.510\n",
       "505       506  ...  0.573\n",
       "\n",
       "[506 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(\"../repository/metadata/training_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb5deb30-3325-4cdf-b3e3-9383a519aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views to \u001b[1m\u001b[32m2022-07-16 20:48:22+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mtoxic_monthly_stats\u001b[0m from \u001b[1m\u001b[32m2022-05-17 20:48:22+00:00\u001b[0m to \u001b[1m\u001b[32m2022-07-16 20:48:22+00:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mtarget_feature_view\u001b[0m from \u001b[1m\u001b[32m2022-05-17 20:48:22+00:00\u001b[0m to \u001b[1m\u001b[32m2022-07-16 20:48:22+00:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "fs.materialize_incremental(end_date=datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03fc314f-6a73-4d7d-b68c-89077ac03ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views from \u001b[1m\u001b[32m2020-08-15 20:51:43+00:00\u001b[0m to \u001b[1m\u001b[32m2022-07-16 20:51:43+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mtoxic_monthly_stats\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 506/506 [00:00<00:00, 13913.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mtarget_feature_view\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 506/506 [00:00<00:00, 15262.43it/s]\n"
     ]
    }
   ],
   "source": [
    "fs.materialize(\n",
    "    end_date=datetime.now(),\n",
    "    start_date=datetime.now() - timedelta(days=700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdbbbb12-c93f-40e8-b8cf-1923002c0a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'house_id': [2, 345, 344],\n",
       " 'NOX': [0.4690000116825104, 0.48399999737739563, 0.48399999737739563]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_response = fs.get_online_features(\n",
    "    features=[\n",
    "        \"toxic_monthly_stats:NOX\",\n",
    "        # \"driver_hourly_stats:acc_rate\",\n",
    "        # \"driver_hourly_stats:avg_daily_trips\",\n",
    "    ],\n",
    "    entity_rows=[{\"house_id\": 2}, {\"house_id\": 345}, {\"house_id\": 344}],\n",
    ")\n",
    "online_response_dict = online_response.to_dict()\n",
    "online_response_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5326f5be-3c15-47db-85d5-95d84a62c4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>345</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>344</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id   NOX\n",
       "0       345  None\n",
       "1       344  None"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining our features names\n",
    "feast_features = [\n",
    "        \"toxic_monthly_stats:NOX\",\n",
    "    ]\n",
    "\n",
    "# Getting the latest features\n",
    "features = fs.get_online_features(\n",
    "    features=feast_features,    \n",
    "    entity_rows=[{\"house_id\": 345}, {\"house_id\": 344}]\n",
    ").to_dict()\n",
    "\n",
    "# Converting the features to a DataFrame\n",
    "features_df = pd.DataFrame.from_dict(data=features)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bda256-c8b9-4ee2-a559-41e8cb60ecbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
